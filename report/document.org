#+LATEX_HEADER: \PassOptionsToPackage{table,xcdraw}{xcolor}

#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{colortbl}
#+LATEX_HEADER: \usepackage{enumitem}
#+LATEX_HEADER: \usepackage[ruled, lined, longend, linesnumbered]{algorithm2e}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage[dvipsnames]{xcolor}
#+LATEX_HEADER: \usepackage{palatino}
#+LATEX_HEADER: \usepackage{palatino}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{pgf}
#+LATEX_HEADER: \usepackage{lmodern}
#+LATEX_HEADER: \usepackage{import}
#+LATEX_HEADER: \usepackage{layouts}
#+LATEX_HEADER: \usepackage{supertabular}
#+LATEX_HEADER: \usepackage{xtab,afterpage}
#+LATEX_HEADER: \usepackage{makecell}

# show section numbers
#+LATEX_HEADER: \setcounter{secnumdepth}{3}
# smaller parskip
\setlength{\parskip}{1pt}
# larger space after shaded boxes
#+LATEX_HEADER: \setlength{\partopsep}{1em}
# argmin/max definitions
#+LATEX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LATEX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LATEX_HEADER: \lstset{basicstyle=\ttfamily, keywordstyle=\bfseries, language=Python, float}
#+LATEX_HEADER: \captionsetup[table]{skip=10pt,justification=centering}
#+LATEX_HEADER: \renewcommand{\arraystretch}{1.4}

#+LATEX_CLASS: IEEEtran
#+LATEX_CLASS_OPTIONS: [a4paper,10pt,compsoc,conference]

#+BIBLIOGRAPHY: gzip.bib
#+CITE_EXPORT: natbib plain 
#+OPTIONS: num:3
#+OPTIONS: toc:nil
#+EXCLUDE_TAGS: noexpo

#+TITLE: On Exploiting gzip's Content-Dependent Compression
#+AUTHOR: Matteo Franzil, Luis Augusto Dias Knob
#+DATE: 2023-05-11
#+LANGUAGE: en

#+MACRO: documenttype report

# \message{ \message{The column width is: \the\columnwidth}
# \message{The text width is: \the\textwidth}
# In inches: \printinunitsof{in}\prntlen{\columnwidth}
# In inches: \printinunitsof{in}\prntlen{\textwidth}

* Introduction

Despite the development of more recent compression techniques like ~bzip2~
and ~xz,~ ~gzip~ is still a well-liked UNIX compression tool. Gzip is
employed as the default compression technique in the majority of Linux and
UNIX distributions. Although it has a somewhat poor compression ratio when
compared to other utilities, this can be justified by its simplicity and
quick compression and decompression times
[cite:@GNUGzip;@deutschDEFLATECompressedData1996]. This is also true for some
tools, like ~containerd~, which employs gzip as its standard compression
technique for its image layers
[cite:@MakeImageLayer;@SupportParallelDecompression].

In this report, we explore the possibility of exploiting gzip's
algorithm for artificially increasing the decompression time of a file. By
creating files filled with semi-random data generated with various methods,
we show that compression and decompression times may vary significantly
depending on the content of the file and the compression level used. Indeed,
we show that the decompression time of a file can be increased by a factor of
3 in the worst case, when compared to the decompression time of a file
containing English text.

* Results

** System setup

We run our experiments on a machine with a 4-core Intel Xeon Silver 4112 CPU
@ 2.60GHz, 64GB of RAM, running Ubuntu Server 20.04.2 LTS. We used ~gzip~
version 1.12 on both machines. Tests were run in isolation and with
CPU pinning, in order to reduce the impact of other processes on the results,
and were run 5 times to reduce the impact of noise.

Our tests comprised the following steps:

1. Generate a file of 100MB in size, with a specific content
2. Compress the file with gzip
3. Decompress the file with gzip

We measured the time taken by each step, the size of the compressed and
uncompressed files, the CPU usage, and the compression ratios. We repeated
this test for each of ~gzip~'s compression levels (1-9). 

** Popular tools

We first started by analyzing the compression and decompression times of
files generated with some popular random data generators. We generated files
with the following tools [cite:@strandbergLorem2022]:

- ~od --format=x /dev/urandom~
- ~base64 /dev/urandom | head -c 1G~
- ~cat /dev/urandom | tr -dc 'a-zA-Z0-9' | head -c 1G~
- ~openssl rand -out myfile "$( echo 1G | numfmt --from=iec )"~
- ~lorem -c 1000000000~

We then compressed and decompressed these files with gzip, using the methods
described above. The decompression times are shown in \autoref{fig:popular-tools}.

\begin{figure}
  \begin{center}
    \input{./drawable/results-v1.pgf}
  \end{center}
  \caption{Decompression times for files generated with popular tools.}
  \label{fig:popular-tools}
\end{figure}

We can see that the decompression times vary significantly
depending on the tool used to generate the file. For example, the file
generated by ~lorem~, containing English text, is decompressed in 3 seconds.
~openssl~ and direct ~/dev/urandom~ output are decompressed in 4.5 seconds.
Finally, the files generated by ~base64~ and ~tr~ require between 7 and 8
seconds to be decompressed, depending on the compression level used. Finally,
~od~'s output is decompressed in 9 seconds. 

** ~od~'s output

Wishing to understand why ~od~'s output was decompressed slower than the
others, we decided to fully leverage ~od~'s various output formats. We thus
generated files with the following output formats:

- ~x~ (hexadecimal)
- ~a~, ~c~ (ASCII both named and unnamed)
- ~d1~, ~d2~, ~d4~, ~d8~ (decimal)
- ~f~ (floating point)
- ~o~ (octal)
- ~u1~, ~u2~, ~u4~, ~u8~ (unsigned decimal)

We decided to use the decimal and unsigned decimal format variants with 1, 2,
4, and 8 bytes in order to see if the size of the numbers (and the minus sign
in decimal formats) had any impact on the decompression time. It must be
remembered we voluntarily un-padded the results of ~od~, and thus, the files
are comprised of a single line of content with no spaces or newlines.

We repeated the same tests as before, and the results are shown in
\autoref{fig:od-output}, although with 100MB files instead of 1GB files, due
to the long time required to generate the files.

\begin{figure}[h!]
  \begin{center}
    \input{./drawable/results-od.pgf}
  \end{center}
  \caption{Decompression times for files generated with ~od~'s various output formats.}
  \label{fig:od-output}
\end{figure}

We can see that the decompression times vary significantly depending on the
output format chosen. For our nefarious purposes, the best output formats (at
level 6, the default one) are ~a~ and ~c~ (ASCII named and unnamed), although
~x~ almost matches them.

** Finding the optimal file

Finally, the graph in \autoref{fig:all} shows the decompression times for
~od~'s best output formats along with the other tools.

\begin{figure}[h]
  \begin{center}
    \input{./drawable/results-v2.pgf}
  \end{center}
  \caption{Decompression times for files generated with ~od~'s best output formats and the other tools.}
  \label{fig:all}
\end{figure}

** Other results

To verify that our results were not specific to our machine, we also ran the
same tests on a MacBook Pro (late 2020) with an M1 CPU, 16GB of RAM, and
running macOS Ventura 13.3.1 (a). We used the same version of ~gzip~ as on
Ubuntu (1.12). Results were similar, although the MacBook Pro was on average
faster than the Ubuntu machine. We thus do not include the results in this
report.

Furthermore, we tried various file sizes, between 100MB and 1GB, in order to
verify there was also no correlation between the file size and the results.
Again, we found that the results were comparable, and thus we only reported
the results once for the 100MB files.


* Conclusions

We have shown that the decompression time of a file can vary significantly
depending on the content of the file and the compression level used. Indeed,
we have shown that the decompression time of a file can be increased by a
factor of 3 when files are filled with datafrom ~od~'s ~a~ and ~c~ output formats,
when compared to the decompression time of a file containing English text.

We believe that this is a serious issue, as it can be used to artificially
increase the decompression time of a file, which can be used to slow down
systems that rely on gzip for decompression. For example, this could be used
to slow down the unpacking of Docker images, which use gzip for compression
of the various layers.

* Bibliography

#+PRINT_BIBLIOGRAPHY:

* Appendix

The following are the full results of our tests for the last graph in the
previous section. 

\input{results-v2.tex}

